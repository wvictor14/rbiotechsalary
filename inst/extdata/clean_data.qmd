---
title: "Cleaning r/biotech Salary Data"
#date: last-modified
date-modified: last-modified
format:
  html:
    toc: true
    toc-depth: 4
editor: visual
editor_options: 
  chunk_output_type: console
---


```{r}
outdir <- here::here('inst', 'extdata')
```

# libraries

```{r, message = FALSE, warning = FALSE}
library(readr)
library(here)
library(purrr)
library(glue)
library(lubridate)
library(forcats)
library(dplyr)
library(tidyr)
library(gt)
library(ggplot2)
library(skimr)
library(janitor)
library(stringr)
library(tictoc)
library(usethis)
```

# preprocess excel data

[link to google sheets](https://docs.google.com/spreadsheets/d/1G0FmJhkOME_sv66hWmhnZS5qR2KMTY7nzkxksv46bfk/edit#gid=491268892)

I extract the sheets into separate csv files so that they can display on github and be more easily used.

```{r process-sheets, eval = FALSE}
# run this once, manually
library(googlesheets4)
google_sheets_url <- 'https://docs.google.com/spreadsheets/d/1G0FmJhkOME_sv66hWmhnZS5qR2KMTY7nzkxksv46bfk/edit#gid=491268892'
sheet_names <- c('2024', '2023', '2022')

#read_sheet(google_sheets_url, sheet = '2024', col_types = 'c')

tic('done reading google sheets')
sheet_names |>  
  # read it as character values to avoid list-column headaches
  walk(
    \(x) read_sheet(google_sheets_url, sheet = x, col_types = 'c') |> 
      write_csv(here(outdir', glue::glue('{x}_survey_results.csv')))
  )
toc()
```

# Initial exploratory

## Completeness

Look at response rate per question (number of NAs):

```{r read_data}
# read in each sheet and combine
sal <- c('2022', '2023', '2024') %>% 
  setNames(., nm = .) |>   
  map(
    \(x) read_csv(
      here(outdir, glue::glue('{x}_survey_results.csv')), 
      col_types = cols(.default = "c"))
  )  |> 
  list_rbind(names_to = 'sheet_year')

# assess NAs
percent_na <- function(x, invert = FALSE) {
  p <- sum(is.na(x)) / length(x)
  if (invert) { p <- 1 - p}
  scales::percent(p)
}

sal |> 
  summarize(
    across(everything(), ~percent_na(.x, invert = TRUE))
  ) |> 
  pivot_longer(everything(), names_to = 'column', values_to = '% responded') |> 
  gt()
```

skimr has a great function that does this, `skim`:

```{r}
sal |>  skim()
```

### save raw data summary

```{r}
sal |> 
  skimr::skim() |> 
  as_tibble() |> 
  rename_with(~stringr::str_remove_all(.x, 'character\\.')) |> 
  select(-skim_type, -whitespace)  |> 
  readr::write_csv(here::here(outdir, 'salary_raw_data.csv'))
```

## Remove those with 0 answers:

```{r}
# remove variables that are all NA:
var_all_na <- sal |>  
  skim()  |>   
  filter(n_missing == nrow(sal)) |> 
  pull(skim_variable)

message('removing variables that are have all missing values')
sal_clean <- sal |> 
  select(-any_of(var_all_na))
message(glue(
  "{length(var_all_na)}/{ncol(sal)} columns removed that are all NA"
))
print(var_all_na)
```

## Rename column names

I rename column names to make them easier to call:

```{r}
# view changed colnames
tibble(original = colnames(sal)) |> 
  mutate(new = janitor::make_clean_names(original)) |> 
  gt() 
sal_clean <- janitor::clean_names(sal_clean)
```

Now I begin cleaning the variable values

# Column cleaning

It would be a massive effort to clean every column. Let's prioritize to most important ones:

-   Salary
-   base
-   target bonus (%)
-   equity
-   Title
-   Experience
-   years
-   degree
-   Location
-   country
-   city

If I have time, can look also at:

-   401k match

## salary

-   base
-   target bonus (%)
-   equity

There are a handful of responses with annual salary reportedly less than \$5000. There is one person that responded with "\$1" - we remove them. And then the remaining report a range from 105-210, which likely represent 105**k** - 210**k**, I multiple these ones by 1000.

### base

```{r}
# make numeric
sal_clean <- sal_clean |>  
  mutate(
    across(
      c(compensation_annual_base_salary_pay, 
        compensation_annual_target_bonus), ~as.numeric(.x))
  )

sal_clean |>  
  ggplot(aes(x = compensation_annual_base_salary_pay)) + 
  geom_histogram() +
  scale_x_continuous(labels = scales::number)

# a bunch of salary entries that are <1000:
sal_clean |> 
  filter(compensation_annual_base_salary_pay < 5000) |> 
  count(compensation_annual_base_salary_pay)

# remove guy with "1" in base
sal_clean_filt <- sal_clean |> 
  filter(compensation_annual_base_salary_pay > 1)
n_removed <- nrow(sal_clean) - nrow(sal_clean_filt)

# 1 guy added extra 0
sal_clean_filt <- sal_clean_filt |> 
  mutate(compensation_annual_base_salary_pay = ifelse(
    compensation_annual_base_salary_pay == 1350000 &
      sheet_year == '2023', 
    1350000/10, 
    compensation_annual_base_salary_pay
  ))

# for the remainder of individuals with salary < 5000 (all in hundreds), 
# multiply by 1000
sal_clean_filt <- sal_clean_filt |>  
  mutate(
    salary_base = ifelse(
      compensation_annual_base_salary_pay < 1000,
      compensation_annual_base_salary_pay*1000, 
      compensation_annual_base_salary_pay     
    ))


n_changed <- sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  nrow() 
```

We removed `r n_removed` data point, and changed / cleaned `r n_changed`

```{r}
sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  select(compensation_annual_base_salary_pay, salary_base) |> 
  gt()
```

### target bonus

is a mix of % and \$ amount

I combine this with annual base to create two new variables: bonus_pct, bonus (raw)

My strategy:

1.  Remove special characters, except periods which represent decimals
2.  convert to numeric, which has two impacts:

-   "Not Applicable/None" entries will be converted to NAs
-   percentages will be converted from those values that are less than "50"
-   raw currency values will be simply those that are above 1000

3.  If there is a range of values, then I keep the higher value, since bonus is typically interpreted as the maximum bonus amount, depending on company and individual performance

```{r}
sal_clean_filt |> 
  select(compensation_annual_base_salary_pay, 
         compensation_annual_target_bonus) |>  
  head(n = 20) |> gt() 

sal_clean_filt <- sal_clean_filt |> 
  mutate(
    bonus_cleaned = compensation_annual_target_bonus |> 
      
      # remove special characters
      str_remove_all('[\\$,\\%]') |>
      
      # remove any non-digits at beginning or end of string
      str_remove('^[^\\d]+') |> 
      str_remove('[^\\d]+$') |> 
      
      # if there is text between numbers, 
      # split it as likely this represents a range
      str_split('[^\\d\\.]+') |> 
      
      # convert to numeric
      # if it is a range, keep the highest value
      map_dbl(\(x) x |> as.numeric() |> max()) 
  ) |> 
  
  mutate(bonus_pct = case_when(
    bonus_cleaned < 50 ~ bonus_cleaned/100,
    bonus_cleaned > 1000 ~ bonus_cleaned / salary_base,
    TRUE ~ 0
  ),
  bonus = case_when(
    bonus_cleaned < 50 ~ bonus_pct * salary_base,
    bonus_cleaned > 1000 ~ bonus_cleaned,
    TRUE ~ 0
  ))
```

Let's look at the cleaned bonus data vs the original:

```{r}
sal_clean_filt |> 
  
  # take 12 rows, 6 that are different, and then 6 not
  mutate(different = compensation_annual_target_bonus != bonus_cleaned) |> 
  slice_sample(n = 6, by = different) |>  
  
  count(salary_base, compensation_annual_target_bonus, 
        bonus_cleaned, bonus_pct, bonus)  |> 
  arrange(salary_base) |> 
  gt() |> 
  fmt_percent(bonus_pct) |> 
  fmt_currency(c(salary_base, bonus))


n_changed_bonus <- sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  nrow() 
```

I converted bonus into % and raw \$. This involved modifying `r n_changed_bonus` bonus data points.

### total comp

```{r}
sal_clean_filt <- sal_clean_filt |> 
  mutate(salary_total = salary_base + bonus)

sal_clean_filt |>  count(is.na(salary_total))
sal_clean_filt |>  count(is.na(salary_base))
sal_clean_filt |>  count(is.na(bonus))
```

### equity

NOT CURRENTLY ATTEMPTED. Prioritize other data first.

Equity is a little bit complicated:

Some are reported as options, some are reported in 1000s e.g. "15k" vs "15000" Some are reported in \$ amount Some say "it varies per year" There are a few that report %, is that amount percentage from base in \$ or in options??

1.  We set "Not Applicable/None" to 0
2.  Set responses that report %, to NA (I don't want to deal with this and it is a small number of responses)
3.  We then create two variables, equity_options, equity_currency_value

-   `equity_currency_value` takes those data points that start with a dollar sign \$
-   `equity_options` is everything else
-   treat this the same as base salary:
-   clean and then:
-   where if it is less than 1000, multiple by 1000

3.  Convert everything to numeric

```{r}
sal_clean_filt |> 
  pull(compensation_annual_equity_stock_option) |>  
  unique() |> 
  head(n = 20) 
```

## Title

Title is very important, but also very messy. Titles can have different meanings (compensation, experience, responsibilities) between companies, and location.

```{r}
sal_clean_filt |> 
  count(role_title_of_current_position) |>
  slice_sample(n=10) |> 
  gt() 
```

### filtering

Remove some obvious rows

```{r}
sal_clean_filt <- sal_clean_filt |> 
  filter(role_title_of_current_position != 'pimp')
```

### Standardize titles

It's a lot of work to standardize titles, so I focus on what I know best, scientist-relaeta Let me intialize some empty columns

```{r}
sal_clean_filt <- sal_clean_filt |> 
  mutate(
    title_category = NA_character_, 
    title_general = NA_character_, 
    title_detail = NA_character_
  )
```

#### Scientist

It's a lot of work to standardize titles, so I focus on what I know best, scientist

Scientist:

-   Research Associate
-   Associate Scientist
-   Scientist
-   Senior Scientist
-   Staff Scientist
-   Principal Scientist

I'm going to create two new variables

-   `title_category` - Set this to `Scientist`
-   `title_general` - A variable that includes general scientist + levels, e.g. Sci **III**, and **Associate** Sci
-   `title_detail` - Contains the other detail of job title, e.g. **Research**, **Clinical**

```{r}
# all roles matching "Scientist"
sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('Scientist', ignore_case = FALSE))) |> 
  count(role_title_of_current_position) |> head(25) |> gt()
```

Try a different approach:

Hierarchical, see case_when

The order of the statements are impactful e.g.

-   If they have associate, they will associate scientists
-   If they have associate and "principal" they will be principal scientist

```{r}
sal_clean_filt <- sal_clean_filt |>
  mutate(
    # title_category
    title_category =  
      ifelse(
        str_detect(
          role_title_of_current_position, regex('Scientist', ignore_case = TRUE)),
        'Scientist',
        title_category
      ),
    
    # scratch
    a = title_category == 'Scientist',
    b = role_title_of_current_position |> 
      str_replace_all('sr\\.?', 'Senior'),
    
    # title detail
    title_detail = ifelse(
      a,
      role_title_of_current_position |> 
        str_remove_all(
          regex('(Associate|Scientist|Principal|Senior)*', ignore_case = TRUE)) |> 
        str_replace_all('[:punct:]+', ' ') # |> 
        # str_replace_all('\\s{2,}', ' ')
      ,
      NA
  )
  ) |> 
  
  # title_gen most specific to most general
  mutate(title_general = case_when(
    
    a & str_detect(
      b, regex('Principal', ignore_case = TRUE)) ~ 'Principal Scientist',
    a & str_detect(
      b, regex('Senior', ignore_case = TRUE)) ~ 'Senior Scientist',
    a & str_detect(
      b, regex('Associate', ignore_case = TRUE)) ~ 'Associate Scientist',
    a ~ 'Scientist',
  )) |> 
  
  # clean
  select(-a, -b)

sal_clean_filt |>  
  count(role_title_of_current_position, title_general, title_detail) |>
  slice_sample(n = 20) |>  gt()

sal_clean_filt |>  count(title_general) |> gt()
```

#### Directors and VP

Same approach as for Scientist, but these are the levels:

```{r}
sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('(Dir(ector)*)|(AD)', ignore_case = TRUE))) |> 
  count(role_title_of_current_position) |>  slice_sample(n = 25) |> gt()
```

```{r}
sal_clean_filt <- sal_clean_filt |>
  mutate(
    
    # title category
    title_category =  
      ifelse(
        str_detect(
          role_title_of_current_position, regex('(Director|\\bAD\\b|\\bDir\\b)', ignore_case = TRUE)),
        'Director',
        title_category
      ),
    
    # scratch for concise coding
    a = title_category == 'Director',
    
    # preprocessed 
    b = role_title_of_current_position |> 
      str_replace_all('sr\\.?', 'Senior') |> 
      str_replace_all('\\bAD\\b', 'Associate Director') |> 
      str_replace_all('\\bAssoc\\.?\\b', 'Associate') |> 
      str_replace_all('\\bDir\\.?\\b', 'Director') 
    ,
    
    # title detail
    title_detail = ifelse(
      a,
      b |> 
        
        # remove the level part + director
        str_remove_all(
          regex('(Associate|Director|Senior|Executive)*', ignore_case = TRUE)) |> 
        str_replace_all('[:punct:]+', ' ') # |> 
        # str_replace_all('\\s{2,}', ' ')
      ,
      title_detail
  )) |> 
  
  # title_gen most specific to most general
  mutate(title_general = case_when(
    
    a & str_detect(
      b, regex('Executive', ignore_case = TRUE)) ~ 'Executive Director',
    a & str_detect(
      b, regex('Senior', ignore_case = TRUE)) ~ 'Senior Director',
    a & str_detect(
      b, regex('Associate', ignore_case = TRUE)) ~ 'Associate Director',
    a ~ 'Director',
    .default = title_general
  )) |> 
  
  # clean
  select(-a, -b) 
sal_clean_filt |> 
  count(
    role_title_of_current_position, title_category, 
    title_general, title_detail) |>
  slice_sample(n = 20) |>  
  gt()

sal_clean_filt |>  
  filter(title_category == 'Director') |>  count(title_general) |> gt()
```

-   Associate Director
-   Director
-   Senior Director
-   Executive Director

#### VP

There's not many VPs

```{r}
sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('(VP|Vice Principal)', ignore_case = TRUE))) |> 
  count(role_title_of_current_position) |> gt()
```

```{r}
sal_clean_filt <- sal_clean_filt |>
  mutate(
    title_category =  
      ifelse(
        str_detect(
          role_title_of_current_position, 
          regex('(VP|Vice Principal)', ignore_case = TRUE)),
        'VP',
        title_category
      ),
    
    # scratch for concise coding
    a = title_category == 'VP',
    
    # preprocessed 
    b = role_title_of_current_position |> 
      str_replace_all(regex('vp', ignore_case = TRUE), 'VP')
    ,
    
    # title detail
    title_detail = ifelse(
      a,
      b |> 
        
        # remove the level part + title_category
        str_remove_all(
          regex('S?VP', ignore_case = TRUE)) |> 
        str_replace_all('[:punct:]+', ' ') |> 
        str_replace_all('\\s{2,}', ' ') |> 
        str_trim()
      ,
      title_detail
    )) |> 
  
  # title_gen most specific to most general
  mutate(title_general = case_when(
    
    a & str_detect(
      b, regex('SVP', ignore_case = TRUE)) ~ 'SVP',
    a ~ 'VP',
    .default = title_general
  )) |> 
  
  # clean
  select(-a, -b)  

sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('vp', ignore_case = TRUE))) |> 
  count(
    role_title_of_current_position, title_category, 
    title_general, title_detail) |>  
  gt()

sal_clean_filt |>  
  filter(title_category == 'VP') |>  count(title_general) |> gt()
```

#### Research Associate

```{r}
sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, 
    regex('(Research Associate|\\bRA\\b)', ignore_case = TRUE)
  )) |> 
  count(role_title_of_current_position) |> gt()
```

```{r}
sal_clean_filt <- sal_clean_filt |>
  mutate(
    # title_category
    title_category =  
      ifelse(
        str_detect(
          role_title_of_current_position, 
          regex('(Research Associate|\\bRA\\b)', ignore_case = TRUE)),
        'Research Associate',
        title_category
      ),
    
    # scratch
    a = title_category == 'Research Associate',
    b = role_title_of_current_position |> 
      str_replace_all('[Ss]r\\.?', 'Senior'),
    
    # title detail
    title_detail = ifelse(
      a,
      role_title_of_current_position |>
        str_remove_all(
          regex('(Research|Associate|Scientist|Senior|\\bRA\\b)*', 
                ignore_case = TRUE)) |> 
        str_replace_all('[:punct:]+', ' ')  |> 
        str_replace_all('\\s{2,}', ' ')
      ,
      title_detail
    )
  ) |> 
  
  # title_gen most specific to most general
  mutate(title_general = case_when(
    a & str_detect(
      b, regex('Senior', ignore_case = TRUE)) ~ 'Senior Research Associate',
    a ~ 'Research Associate',
    .default = title_general
  )) |> 
  
  # clean
  select(-a, -b)

sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('(Research Associate|\\bRA\\b)', ignore_case = TRUE))) |> 
  count(
    role_title_of_current_position, title_category, 
    title_general, title_detail) |>  
  gt()

sal_clean_filt |>  
  filter(title_category == 'Research Associate') |>  count(title_general) |> gt()
```

##### filter

I filter out an outlier sample, looks like they incorrectly inputted bonus:

```{r}
n_1 <- nrow(sal_clean_filt)
sal_clean_filt <- sal_clean_filt |> 
  dplyr::filter(!(
    str_detect(timestamp, '10/31/2023') & 
      role_title_of_current_position == 'Sr Clinical Research Associate')
  )
.diff <- nrow(sal_clean_filt) - n_1
stopifnot(.diff == -1)
```

### Roles that have not been processed

Look at the remaining roles that I have not tried to process.

```{r}
sal_clean_filt |>  
  filter(is.na(title_category)) |> 
  pull(role_title_of_current_position) |>  unique()
```

### Final filtering

The non-processed data is useful, if messy.

For now, I remove rows with title data that I have not cleaned

So basically we only have scientist, directors, and VPs right now.

```{r}
n_before_filter <- nrow(sal_clean_filt)
sal_clean_filt <- sal_clean_filt |> 
  filter(!is.na(title_category))
n_after_filter <- nrow(sal_clean_filt)

p_filt <- scales::percent(n_after_filter / n_before_filter, accuracy = 1)
message(glue("{n_after_filter} / {n_before_filter}  ({p_filt}) remain after filtering for uncleaned title data"))
```

## Experience

```{r}
sal_clean_filt |> 
  count(years_of_experience) |> gt()
```

only 1 will be converted to NA

```{r}
n_before <- sum(is.na(sal_clean_filt$years_of_experience)) 
sal_clean_filt <- sal_clean_filt |> 
  mutate(years_of_experience = as.numeric(years_of_experience)) 
n_after <- sum(is.na(sal_clean_filt$years_of_experience))
message(glue('# NAs\nbefore: {n_before}\nafter: {n_after}'))
```

#### degree

```{r}
sal_clean_filt |> 
  count(what_degrees_do_you_have) |>  gt()
```

contains all degrees, but let's simplify to the highest degree:

e.g.

-   "High School or Equivalent, Bachelors or Equivalent, Masters or Equivalent, PhD or Equivalent"
-   "Bachelors or Equivalent, Masters or Equivalent, PhD or Equivalent"
-   "Bachelors or Equivalent, PhD or Equivalent"
-   "PhD or Equivalent"

should all just be "PhD"

Below code assumes highest degree is the last listed degree

```{r}
sal_clean_filt <- sal_clean_filt |> 
  mutate(experience_highest_degree =  str_extract(
    what_degrees_do_you_have,
    '(?<=,|^)[^,]+$'
  ) |> 
    str_replace('M\\.D\\.', 'MD') |> 
    str_remove('or Equivalent') |> 
    str_replace('MD/Pharm.*', 'MD')
  ) 

# View Changes
sal_clean_filt |>  
  count(is.na(experience_highest_degree), is.na(what_degrees_do_you_have))

sal_clean_filt |>
  count(experience_highest_degree, what_degrees_do_you_have) |>  
  gt()
```

## Location

```{r}
sal_clean_filt |> 
  count(where_is_the_closest_major_city_or_hub, where_are_you_located) |> 
  gt()

sal_clean_filt |> 
  count(
    where_are_you_located, what_country_do_you_work_in,
    where_is_the_closest_major_city_or_hub) |>  
  gt()
```

Location is quite messy data consisting of free-form responses spread between 3 variables:

-   `where_are_you_located` contains Countries (USA, Canada, etc.), states (CO, Carolinas, etc.)

This is not a required question so there are 174 responses that did not respond, and this also included an "other" freeform option.

-   Pharma Central (NY, NJ, PA)
-   New England (MA, CT, RI, NH, VT, ME)
-   DC Metro Area (DC, VA, MD, DE)
-   Carolinas & Southeast (From NC to AR, South FL and LA)
-   Midwest (From OH to KS, North to ND)
-   South & Mountain West (TX to AZ, North to MT)
-   West Coast (California & Pacific Northwest)
-   Other US Location (HI, AK, PR, etc.)
-   Canada
-   United Kingdom and Ireland
-   Germany
-   Other:

The 174 missing responses have values for these other two variables. I think this is due to the survey changing the question at some point, probably due to feedback.

-   `what_country_do_you_work_in` seems to all be countries, but not harmonized e.g.: Usa, usa, United States
-   `where_is_the_closest_major_city_or_hub` mixed bag: Bay area, Boston, san diego, Research Triangle,

Here I create one new location variable `location_country`, that, unlike the other variables, cover \>90% of the data

```{r}
USA <- c(
  "Pharma Central (NY, NJ, PA)",
  "New England (MA, CT, RI, NH, VT, ME)",
  "DC Metro Area (DC, VA, MD, DE)",
  "Carolinas & Southeast (From NC to AR, South FL and LA)",
  "Midwest (From OH to KS, North to ND)",
  "South & Mountain West (TX to AZ, North to MT)",
  "West Coast (California & Pacific Northwest)",
  "Other US Location (HI, AK, PR, etc.)"
)

usa_locations <- c(
  'baltimore',
  'boston',
  'bay area', 
  'baltimore',
  'boulder',
  'california',
  'cambridge',
  'carlsbad',
  'jersey', 'maine',
  'chicago',
  '\\bdc\\b',
  'denver',
  'durham',
  'hayward',
  'hungary',
  'indianapolis',
  'kansas',
  '\\bla\\b',
  'los angeles',
  'maryland',
  'midwest',
  'new york',
  '\\bnj\\b',
  '\\bny[c]?\\b',
  'ohio',
  'philadelphia',
  'phoenix',
  'pittsburgh',
  'raleigh',
  'san diego',
  'ridgefield',
  'rockville',
  'rtp\\b',
  'sacramento',
  'salt lake',
  'seattle',
  'san francisco',
  'socal',
  'united states',
  'u\\.s\\.a',
  'washington'
) 

usa_locations <- str_c('(', str_c(usa_locations, collapse = '|'), ')')

sal_clean_filt <- sal_clean_filt |> 
  mutate(
    
    # prep raw location data
    location_raw_data = case_when(
      !is.na(what_country_do_you_work_in) ~ what_country_do_you_work_in,
      !is.na(where_are_you_located) ~ where_are_you_located, 
      !is.na(where_is_the_closest_major_city_or_hub) ~ where_is_the_closest_major_city_or_hub,
      .default = where_is_the_closest_major_city_or_hub
    ) |> 
      tolower(),
    
    # matching
    location_country = case_when(
      
      ### usa 
      location_raw_data %in% c(
        tolower(USA), 
        'co', 
        'san diego, ca', 
        'united states'
      ) ~ 'United States of America',
      
      location_raw_data |> 
        str_detect(regex('usa?', ignore_case = TRUE)) ~ 
        'United States of America', 
      
      location_raw_data |>
        str_detect(regex(usa_locations, ignore_case = TRUE), ) ~ 
        'United States of America',
      ### UK
      location_raw_data |> 
        str_detect('(u\\.k|dundee|ireland)') ~ 'United Kingdom',
      location_raw_data %in% c(
        'united kingdom and ireland', 'uk') ~ 'United Kingdom',
      
      ### other
      location_raw_data %in% 
        c('canada', 'united states of america',
          'belgium', 'france', 'spain', 'united kingdom', 'benelux', 'germany',
          'sweden', 'switzerland', 'denmark',
          'singapore', 'india', 'australia') ~ location_raw_data,
      
      location_raw_data |>  str_detect('toronto') ~ 'Canada',
      TRUE ~ NA
      
    ),
    location_country = str_to_title(location_country)
  )

sal_clean_filt |> 
  count(location_country, location_raw_data,
        where_are_you_located, what_country_do_you_work_in, 
        where_is_the_closest_major_city_or_hub) |> 
  arrange(location_country) |>  gt()
```

### location granular

for selecting within country:

```{r}
sal_clean_filt <- sal_clean_filt |> 
  mutate(
    location_granular = 
      coalesce(where_is_the_closest_major_city_or_hub, where_are_you_located)
  )  
sal_clean_filt |> 
  count(
    location_granular, 
    where_is_the_closest_major_city_or_hub, 
    where_are_you_located) |>  gt()
```

# Completeness

Compare the missingness of original location variables vs the cleaned `location_country` variable:

```{r}
sal_clean_filt |> 
  summarize(
    across(c(
      location_country,
      location_granular,
      where_are_you_located, 
      what_country_do_you_work_in, 
      where_is_the_closest_major_city_or_hub), 
      ~ percent_na(.x))
  ) |> 
  pivot_longer(everything(), names_to = 'column', values_to = '% missing')
```

# time

```{r}
sal_clean_filt <- sal_clean_filt |> 
  mutate(timestamp = mdy_hms(timestamp),
         year = year(timestamp),
         month = month(timestamp),
         day = day(timestamp)) |> 
  select(timestamp, year:day, everything())
```

# write out the data

```{r}
sal_clean_filt |>
  mutate(date = str_extract(as.character(timestamp), "^[-\\w]+")) |> 
  select(
    date, 
    location_country, 
    title_category, title_general, title_detail,
    salary_base, bonus_pct, bonus,
    
    # need to fix these
    years_of_experience, what_degrees_do_you_have,
    
    # the original data
    where_are_you_located, what_country_do_you_work_in,
    role_title_of_current_position,
    everything(),
    -timestamp, -bonus_cleaned, -year, -month, -day
  ) |> 
  write_csv(here::here(outdir , 'salary_results_cleaned.csv'))
```

kept for historical, will remove:

```{r eval = FALSE}
salaries <- readr::read_csv(here::here('data', 'salary_results_cleaned.csv'))
salaries <- salaries |>  
  mutate(
    title_general = title_general |>
      forcats::fct() |>
      forcats::fct_relevel(
        
        'Research Associate', 'Senior Research Associate',
        
        'Associate Scientist', 'Scientist', 
        'Senior Scientist', 'Principal Scientist',
        
        'Associate Director', 'Director', 'Senior Director', 
        'Executive Director',
        
        'VP', 'SVP'
      ),
    
    title_category = title_category |> 
      forcats::fct() |> 
      forcats::fct_relevel(
        'Research Associate', 'Scientist', 'Director', 'VP'
      ),
    
    location_granular = forcats::fct(location_granular) |> 
      forcats::fct_relevel(USA)
  )
```
